{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 02:04:02.963061: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n",
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.keras.backend.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def mse_loss(static, moving):\n",
    "    \"\"\"Computes the mean squared error (MSE) loss.\n",
    "\n",
    "    Currently, only 4-D inputs are supported.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    static : tf.Tensor, shape (N, H, W, C)\n",
    "        The static image to which the moving image is aligned.\n",
    "    moving : tf.Tensor, shape (N, H, W, C)\n",
    "        The moving image, the same shape as the static image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : tf.Tensor, shape ()\n",
    "        Mean squared error between the static and the moving images,\n",
    "        averaged over the batch.\n",
    "    \"\"\"\n",
    "    loss = tf.reduce_mean(tf.square(moving - static))  # shape ()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def ncc_loss(static, moving):\n",
    "    \"\"\"Computes the normalized cross-correlation (NCC) loss.\n",
    "\n",
    "    Currently, only 4-D inputs are supported.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    static : tf.Tensor, shape (N, H, W, C)\n",
    "        The static image to which the moving image is aligned.\n",
    "    moving : tf.Tensor, shape (N, H, W, C)\n",
    "        The moving image, the same shape as the static image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : tf.Tensor, shape ()\n",
    "        Normalized cross-correlation loss between the static and the\n",
    "        moving images, averaged over the batch. Range is [-1.0, 1.0].\n",
    "        The best value is -1 (perfect match) and the worst is 1.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] `Wikipedia entry for the Cross-correlation\n",
    "           <https://en.wikipedia.org/wiki/Cross-correlation>`_\n",
    "    \"\"\"\n",
    "    eps = tf.constant(1e-9, 'float32')\n",
    "\n",
    "    static_mean = tf.reduce_mean(static, axis=[1, 2], keepdims=True)\n",
    "    moving_mean = tf.reduce_mean(moving, axis=[1, 2], keepdims=True)\n",
    "    # shape (N, 1, 1, C)\n",
    "\n",
    "    static_std = tf.math.reduce_std(static, axis=[1, 2], keepdims=True)\n",
    "    moving_std = tf.math.reduce_std(moving, axis=[1, 2], keepdims=True)\n",
    "    # shape (N, 1, 1, C)\n",
    "\n",
    "    static_hat = (static - static_mean)/(static_std + eps)\n",
    "    moving_hat = (moving - moving_mean)/(moving_std + eps)\n",
    "    # shape (N, H, W, C)\n",
    "\n",
    "    ncc = tf.reduce_mean(static_hat * moving_hat)  # shape ()\n",
    "    loss = -ncc\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn(input_shape=(32, 32, 2)):\n",
    "    \"\"\"Creates a 2-D convolutional encoder-decoder network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : sequence of ints, optional\n",
    "        Input data shape of the form (H, W, C). Default is (32, 32, 2).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model\n",
    "        An instance of Keras' Model class.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Given a concatenated pair of static and moving images as input, the\n",
    "    CNN computes a dense displacement field that is used to warp the\n",
    "    moving image to match with the static image.\n",
    "\n",
    "    The number of channels in the output (displacement field) is equal\n",
    "    to the dimensionality of the input data. For 3-D volumes, it is 3,\n",
    "    and for 2-D images, it is 2. The first channel comprises\n",
    "    displacement in the x-direction and the second comprises\n",
    "    displacement in the y-direction.\n",
    "    \"\"\"\n",
    "    out_channels = 2\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # encoder\n",
    "    x = layers.Conv2D(32, kernel_size=3, strides=2, padding='same',\n",
    "                      activation='relu')(inputs)            # 32 --> 16\n",
    "    x = layers.BatchNormalization()(x)                      # 16\n",
    "    x = layers.Conv2D(32, kernel_size=3, strides=1, padding='same',\n",
    "                      activation='relu')(x)                 # 16\n",
    "    x = layers.BatchNormalization()(x)                      # 16\n",
    "    x = layers.MaxPool2D(pool_size=2)(x)                    # 16 --> 8\n",
    "    x = layers.Conv2D(64, kernel_size=3, strides=1, padding='same',\n",
    "                      activation='relu')(x)                 # 8\n",
    "    x = layers.BatchNormalization()(x)                      # 8\n",
    "    x = layers.Conv2D(64, kernel_size=3, strides=1, padding='same',\n",
    "                      activation='relu')(x)                 # 8\n",
    "    x = layers.BatchNormalization()(x)                      # 8\n",
    "    x = layers.MaxPool2D(pool_size=2)(x)                    # 8 --> 4\n",
    "    x = layers.Conv2D(128, kernel_size=3, strides=1, padding='same',\n",
    "                      activation='relu')(x)                 # 4\n",
    "    x = layers.BatchNormalization()(x)                      # 4\n",
    "\n",
    "    # decoder\n",
    "    x = layers.Conv2DTranspose(64, kernel_size=2, strides=2,\n",
    "                               padding='same')(x)           # 4 --> 8\n",
    "    x = layers.Conv2D(64, kernel_size=3, strides=1, padding='same',\n",
    "                      activation='relu')(x)                 # 8\n",
    "    x = layers.BatchNormalization()(x)                      # 8\n",
    "    x = layers.Conv2DTranspose(32, kernel_size=2, strides=2,\n",
    "                               padding='same')(x)           # 8 --> 16\n",
    "    x = layers.Conv2D(32, kernel_size=3, strides=1, padding='same',\n",
    "                      activation='relu')(x)                 # 16\n",
    "    x = layers.BatchNormalization()(x)                      # 16\n",
    "    x = layers.Conv2DTranspose(16, kernel_size=2, strides=2,\n",
    "                               padding='same')(x)           # 16 --> 32\n",
    "    x = layers.Conv2D(16, kernel_size=3, strides=1, padding='same',\n",
    "                      activation='relu')(x)                 # 32\n",
    "    x = layers.BatchNormalization()(x)                      # 32\n",
    "    x = layers.Conv2D(out_channels, kernel_size=1, strides=1,\n",
    "                      padding='same')(x)                    # 32\n",
    "\n",
    "    # Create the model.\n",
    "    model = tf.keras.Model(inputs, x, name='simple_cnn')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simple_cnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 2)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 16, 16, 32)        608       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 16, 16, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 16, 16, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 8, 8, 32)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 8, 8, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 8, 8, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 4, 4, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 4, 4, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 8, 8, 64)          32832     \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 8, 8, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 16, 16, 32)        8224      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 16, 16, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 32, 32, 16)        2064      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 32, 32, 16)        2320      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 32, 32, 16)        64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 32, 32, 2)         34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 232514 (908.26 KB)\n",
      "Trainable params: 231650 (904.88 KB)\n",
      "Non-trainable params: 864 (3.38 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = simple_cnn()\n",
    "model.summary()\n",
    "# tf.keras.utils.plot_model(model, show_shapes=True, dpi=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def grid_sample(moving, grid):\n",
    "    \"\"\"Given a moving image and a sampling grid as input, computes the\n",
    "    transformed image by sampling the moving image at locations given by\n",
    "    the grid.\n",
    "\n",
    "    Currently, only 2-D images, i.e., 4-D inputs are supported.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    moving : tf.Tensor, shape (N, H, W, C)\n",
    "        The moving image.\n",
    "    grid : tf.Tensor, shape (N, H, W, C)\n",
    "        A tensor of sampling points (x, y). The x and y values should be\n",
    "        normalized to [-1.0, 1.0] range.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    moved : tf.Tensor, shape (N, H, W, C)\n",
    "        The transformed image.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Let M be the moving image of shape (H, W, C), T be the transformed\n",
    "    image of the same shape and G be the 2-D sampling grid of shape\n",
    "    (H, W, 2). The value of T at a location (x, y) is T[y, x, :] =\n",
    "    M[y', x', :] where [x', y'] = G[y, x, :].\n",
    "\n",
    "    Further, [x', y'] = [x + dx, y + dy] where [dx, dy] are the\n",
    "    displacements outputted by the CNN. When dx and dy are 0, the\n",
    "    sampling grid G is a regular grid and the transformed image is the\n",
    "    same as the moving image.\n",
    "\n",
    "    Since the sampling point (x + dx, y + dy) can be non-integral, the\n",
    "    value M[y', x'] is calculated using bi-linear interpolation.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] `Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. \"Spatial\n",
    "        transformer networks.\" Advances in neural information processing\n",
    "        systems. 2015. <https://arxiv.org/abs/1506.02025>`_\n",
    "    .. [2] `TensorFlow implementation of spatial transformer networks.\n",
    "        <https://github.com/tensorflow/models/tree/master/research/transformer>`_\n",
    "    .. [3] `Spatial Transformer Networks by Kushagra Bhatnagar\n",
    "        <https://link.medium.com/0b2OrmqVO5>`_\n",
    "    \"\"\"\n",
    "    nb, nh, nw, nc = moving.shape\n",
    "\n",
    "    x = grid[..., 0]  # shape (N, H, W)\n",
    "    y = grid[..., 1]\n",
    "    x = tf.cast(x, 'float32')\n",
    "    y = tf.cast(y, 'float32')\n",
    "\n",
    "    # Scale x and y from [-1.0, 1.0] to [0, W] and [0, H] respectively.\n",
    "    x = (x + 1.0) * 0.5 * tf.cast(nw, 'float32')\n",
    "    y = (y + 1.0) * 0.5 * tf.cast(nh, 'float32')\n",
    "\n",
    "    y_max = tf.cast(nh - 1, 'int32')\n",
    "    x_max = tf.cast(nw - 1, 'int32')\n",
    "    zero = tf.constant(0, 'int32')\n",
    "\n",
    "    # The value at (x, y) is a weighted average of the values at the\n",
    "    # four nearest integer locations: (x0, y0), (x1, y0), (x0, y1) and\n",
    "    # (x1, y1) where x0 = floor(x), x1 = ceil(x).\n",
    "    x0 = tf.cast(tf.floor(x), 'int32')\n",
    "    x1 = x0 + 1\n",
    "    y0 = tf.cast(tf.floor(y), 'int32')\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    # Make sure indices are within the boundaries of the image.\n",
    "    x0 = tf.clip_by_value(x0, zero, x_max)\n",
    "    x1 = tf.clip_by_value(x1, zero, x_max)\n",
    "    y0 = tf.clip_by_value(y0, zero, y_max)\n",
    "    y1 = tf.clip_by_value(y1, zero, y_max)\n",
    "\n",
    "    # Collect indices of the four corners.\n",
    "    b = tf.ones_like(x0) * tf.reshape(tf.range(nb), [nb, 1, 1])\n",
    "    idx_a = tf.stack([b, y0, x0], axis=-1)  # all top-left corners\n",
    "    idx_b = tf.stack([b, y1, x0], axis=-1)  # all bottom-left corners\n",
    "    idx_c = tf.stack([b, y0, x1], axis=-1)  # all top-right corners\n",
    "    idx_d = tf.stack([b, y1, x1], axis=-1)  # all bottom-right corners\n",
    "    # shape (N, H, W, 3)\n",
    "\n",
    "    # Collect values at the corners.\n",
    "    moving_a = tf.gather_nd(moving, idx_a)  # all top-left values\n",
    "    moving_b = tf.gather_nd(moving, idx_b)  # all bottom-left values\n",
    "    moving_c = tf.gather_nd(moving, idx_c)  # all top-right values\n",
    "    moving_d = tf.gather_nd(moving, idx_d)  # all bottom-right values\n",
    "    # shape (N, H, W, C)\n",
    "\n",
    "    x0_f = tf.cast(x0, 'float32')\n",
    "    x1_f = tf.cast(x1, 'float32')\n",
    "    y0_f = tf.cast(y0, 'float32')\n",
    "    y1_f = tf.cast(y1, 'float32')\n",
    "\n",
    "    # Calculate the weights.\n",
    "    wa = tf.expand_dims((x1_f - x) * (y1_f - y), axis=-1)\n",
    "    wb = tf.expand_dims((x1_f - x) * (y - y0_f), axis=-1)\n",
    "    wc = tf.expand_dims((x - x0_f) * (y1_f - y), axis=-1)\n",
    "    wd = tf.expand_dims((x - x0_f) * (y - y0_f), axis=-1)\n",
    "\n",
    "    # Calculate the weighted sum.\n",
    "    moved = tf.add_n([wa * moving_a, wb * moving_b, wc * moving_c,\n",
    "                      wd * moving_d])\n",
    "    return moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def regular_grid(shape):\n",
    "    \"\"\"Returns a batch of 2-D regular grids.\n",
    "\n",
    "    Currently, only 2-D regular grids are supported.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    shape : sequence of ints, shape (3, )\n",
    "        The desired regular grid shape of the form (N, H, W).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grid : tf.Tensor, shape (N, H, W, 2)\n",
    "        A batch of 2-D regular grids, values normalized to [-1.0, 1.0]\n",
    "        range.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Sampling using the regular grid is an identity transformation, i.e.,\n",
    "    it results in the same input and output images.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] `NumPy, \"numpy.meshgrid\"\n",
    "        <https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html>`_\n",
    "    .. [2] `NumPy, \"numpy.indices\"\n",
    "        <https://numpy.org/doc/stable/reference/generated/numpy.indices.html>`_\n",
    "    \"\"\"\n",
    "    nb, nh, nw = shape\n",
    "\n",
    "    x = tf.linspace(-1.0, 1.0, nw)  # shape (W, )\n",
    "    y = tf.linspace(-1.0, 1.0, nh)  # shape (H, )\n",
    "\n",
    "    X, Y = tf.meshgrid(x, y)  # shape (H, W), both X and Y\n",
    "\n",
    "    grid = tf.stack([X, Y], axis=-1)\n",
    "    grid = tf.expand_dims(grid, axis=0)  # shape (1, H, W, 2)\n",
    "\n",
    "    # Repeat the grids along the batch dim.\n",
    "    multiples = tf.constant([nb, 1, 1, 1], tf.int32)\n",
    "    grid = tf.tile(grid, multiples)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, moving, static, criterion, optimizer):\n",
    "    \"\"\"A generic training procedure for one iteration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        A convolutional encoder-decoder network.\n",
    "    moving : tf.Tensor, shape (N, H, W, C)\n",
    "        A batch of moving images.\n",
    "    static : tf.Tensor, shape (1, H, W, C)\n",
    "        The static image.\n",
    "    criterion\n",
    "        The loss function.\n",
    "    optimizer\n",
    "        An optimzer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : tf.Tensor, shape ()\n",
    "        The average loss for the batch.\n",
    "    \"\"\"\n",
    "    nb, nh, nw, nc = moving.shape\n",
    "\n",
    "    # Repeat the static image along the batch dim.\n",
    "    multiples = tf.constant([nb, 1, 1, 1], tf.int32)\n",
    "    static = tf.tile(static, multiples)\n",
    "\n",
    "    # Define the GradientTape context for automatic differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get the deformation field\n",
    "        inputs = tf.concat([moving, static], axis=-1)\n",
    "        deformation = model(inputs)\n",
    "\n",
    "        # Compute the new sampling grid.\n",
    "        grid = regular_grid([nb, nh, nw])\n",
    "        grid_new = grid + deformation\n",
    "        grid_new = tf.clip_by_value(grid_new, -1, 1)\n",
    "\n",
    "        # Sample the moving image using the new sampling grid.\n",
    "        moved = grid_sample(moving, grid_new)\n",
    "\n",
    "        # Compute the loss.\n",
    "        loss = criterion(moved, static)\n",
    "\n",
    "    # Compute gradients.\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    # Update the trainable parameters.\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(model, moving, static, criterion):\n",
    "    \"\"\"A generic testing procedure.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        A convolutional encoder-decoder network.\n",
    "    moving : tf.Tensor, shape (N, H, W, C)\n",
    "        A batch of moving images.\n",
    "    static : tf.Tensor, shape (1, H, W, C)\n",
    "        The static image.\n",
    "    criterion\n",
    "        The loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : tf.Tensor, shape ()\n",
    "        The average loss for the batch.\n",
    "    \"\"\"\n",
    "    nb, nh, nw, nc = moving.shape\n",
    "\n",
    "    # Repeat the static image along the batch dim.\n",
    "    multiples = tf.constant([nb, 1, 1, 1], tf.int32)\n",
    "    static = tf.tile(static, multiples)\n",
    "\n",
    "    # Get the deformation field.\n",
    "    inputs = tf.concat([moving, static], axis=-1)\n",
    "    deformation = model(inputs, training=False)\n",
    "\n",
    "    # Compute the new sampling grid.\n",
    "    grid = regular_grid([nb, nh, nw])\n",
    "    grid_new = grid + deformation\n",
    "    grid_new = tf.clip_by_value(grid_new, -1, 1)\n",
    "\n",
    "    # Sample the moving image using the new sampling grid.\n",
    "    moved = grid_sample(moving, grid_new)\n",
    "\n",
    "    # Compute the loss.\n",
    "    loss = criterion(moved, static)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(label=2):\n",
    "    \"\"\"Loads the MNIST dataset and preprocesses it: scales to [0.0, 1.0]\n",
    "    range, resizes the images from (28, 28) to (32, 32) and filters the\n",
    "    dataset to keep images of just one class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    label : {2, 0, 1, 3, 4, 5, 6, 7, 8, 9}, default 2\n",
    "        The class of images to train and test on.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (x_train, x_test) : tuple of ndarrays\n",
    "        NumPy arrays of training and testing images.\n",
    "    \"\"\"\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Discard digits which are not equal to label.\n",
    "    ids_train = np.where(y_train == label)\n",
    "    ids_test = np.where(y_test == label)\n",
    "\n",
    "    x_train = x_train[ids_train]\n",
    "    x_test = x_test[ids_test]\n",
    "\n",
    "    # Scale the image to [0, 1] range.\n",
    "    x_train = x_train.astype(np.float32) / 255.0\n",
    "    x_test = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "    # Add the channel dim at the end. (N, H, W) --> (N, H, W, 1)\n",
    "    x_train = x_train[..., None]\n",
    "    x_test = x_test[..., None]\n",
    "\n",
    "    # Resize images from (28, 28) to (32, 32).\n",
    "    x_train = tf.image.resize(x_train, (32, 32))\n",
    "    x_test = tf.image.resize(x_test, (32, 32))\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(model, moving, static):\n",
    "    \"\"\"Visualize some images after training.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        The trained model.\n",
    "    moving : tf.Tensor, shape (N, H, W, C)\n",
    "        A batch of moving images.\n",
    "    static : tf.Tensor, shape (1, H, W, C)\n",
    "        The static image.\n",
    "    \"\"\"\n",
    "    nb, nh, nw, nc = moving.shape\n",
    "\n",
    "    # Repeat the static image along the batch dim.\n",
    "    multiples = tf.constant([nb, 1, 1, 1], tf.int32)\n",
    "    static = tf.tile(static, multiples)\n",
    "\n",
    "    # Get the deformation fields for the batch.\n",
    "    inputs = tf.concat([moving, static], axis=-1)\n",
    "    deformation = model(inputs, training=False)\n",
    "\n",
    "    # Compute the new sampling grids.\n",
    "    grid = regular_grid([nb, nh, nw])\n",
    "    grid_new = grid + deformation\n",
    "    grid_new = tf.clip_by_value(grid_new, -1, 1)\n",
    "\n",
    "    # Sample the moving images using the new sampling grids.\n",
    "    moved = grid_sample(moving, grid_new)\n",
    "\n",
    "    # Convert the tensors to 8-bit images.\n",
    "    moved = moved.numpy().squeeze(axis=-1) * 255.0\n",
    "    moved = moved.astype(np.uint8)\n",
    "    moving = moving.numpy().squeeze(axis=-1) * 255.0\n",
    "    moving = moving.astype(np.uint8)\n",
    "    static = static.numpy().squeeze(axis=-1) * 255.0\n",
    "    static = static.astype(np.uint8)\n",
    "\n",
    "    # Plot images.\n",
    "    fig = plt.figure(figsize=(3 * 1.7, nb * 1.7))\n",
    "    titles_list = ['Static', 'Moved', 'Moving']\n",
    "    images_list = [static, moved, moving]\n",
    "    for i in range(nb):\n",
    "        for j in range(3):\n",
    "            ax = fig.add_subplot(nb, 3, i * 3 + j + 1)\n",
    "            if i == 0:\n",
    "                ax.set_title(titles_list[j], fontsize=20)\n",
    "            ax.set_axis_off()\n",
    "            ax.imshow(images_list[j][i], cmap='gray')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "\n",
    "    # Load preprocessed training and testing data.\n",
    "    x_train, x_test = load_data(label=args.label)\n",
    "\n",
    "    # Randomly select an image as the static image from the test set.\n",
    "    # idx = np.random.randint(x_test.shape[0])\n",
    "    # static = tf.expand_dims(x_test[idx], axis=0)\n",
    "    static = tf.expand_dims(x_test[0], axis=0)\n",
    "\n",
    "    # Select some images from the test set to show sample results.\n",
    "    # ids = tf.constant(np.random.choice(x_test.shape[0], replace=False,\n",
    "    #                                    size=args.num_samples))\n",
    "    # x_sample = tf.gather(x_test, ids)\n",
    "    x_sample = x_test[:args.num_samples]\n",
    "\n",
    "    # Shuffle and batch the dataset.\n",
    "    from_tensor_slices = tf.data.Dataset.from_tensor_slices\n",
    "    # x_train = from_tensor_slices(x_train).shuffle(10000).batch(args.batch_size)\n",
    "    # x_test = from_tensor_slices(x_test).shuffle(10000).batch(args.batch_size)\n",
    "    x_train = from_tensor_slices(x_train).batch(args.batch_size)\n",
    "    x_test = from_tensor_slices(x_test).batch(args.batch_size)\n",
    "\n",
    "    # Create a model instance.\n",
    "    model = simple_cnn(input_shape=(32, 32, 2))\n",
    "\n",
    "    # Select optimizer and loss function.\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=args.lr)\n",
    "    criterion = ncc_loss # normalized_cross_correlation_loss()  # or mse_loss\n",
    "\n",
    "    # Define the metrics to track training and testing losses.\n",
    "    m_train = tf.keras.metrics.Mean(name='loss_train')\n",
    "    m_test = tf.keras.metrics.Mean(name='loss_test')\n",
    "\n",
    "    # Train and evaluate the model.\n",
    "    for epoch in range(args.epochs):\n",
    "        m_train.reset_states()\n",
    "        m_test.reset_states()\n",
    "        for i, moving in enumerate(x_train):\n",
    "            loss_train = train_step(model, moving, static, criterion,\n",
    "                                    optimizer)\n",
    "            m_train.update_state(loss_train)\n",
    "\n",
    "        for i, moving in enumerate(x_test):\n",
    "            loss_test = test_step(model, moving, static, criterion)\n",
    "            m_test.update_state(loss_test)\n",
    "\n",
    "        print('Epoch: %3d/%d\\tTrain Loss: %.6f\\tTest Loss: %.6f'\n",
    "              % (epoch + 1, args.epochs, m_train.result(), m_test.result()))\n",
    "    print('\\n')\n",
    "\n",
    "    # Show sample results.\n",
    "    plot_images(model, x_sample, static)\n",
    "\n",
    "    # Save the trained model.å\n",
    "    if args.save_model:\n",
    "        model.save('/Users/original/Desktop/Johnson_Lab/image_registration/saved_models/simple_cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/50\tTrain Loss: -0.574216\tTest Loss: -0.587038\n",
      "Epoch:   2/50\tTrain Loss: -0.589611\tTest Loss: -0.607349\n",
      "Epoch:   3/50\tTrain Loss: -0.612260\tTest Loss: -0.630868\n",
      "Epoch:   4/50\tTrain Loss: -0.638424\tTest Loss: -0.660601\n",
      "Epoch:   5/50\tTrain Loss: -0.684421\tTest Loss: -0.702909\n",
      "Epoch:   6/50\tTrain Loss: -0.777029\tTest Loss: -0.824318\n",
      "Epoch:   7/50\tTrain Loss: -0.860147\tTest Loss: -0.748679\n",
      "Epoch:   8/50\tTrain Loss: -0.899604\tTest Loss: -0.877625\n",
      "Epoch:   9/50\tTrain Loss: -0.915796\tTest Loss: -0.893248\n",
      "Epoch:  10/50\tTrain Loss: -0.926911\tTest Loss: -0.918323\n",
      "Epoch:  11/50\tTrain Loss: -0.933641\tTest Loss: -0.928313\n",
      "Epoch:  12/50\tTrain Loss: -0.938783\tTest Loss: -0.939797\n",
      "Epoch:  13/50\tTrain Loss: -0.941757\tTest Loss: -0.943829\n",
      "Epoch:  14/50\tTrain Loss: -0.944696\tTest Loss: -0.947374\n",
      "Epoch:  15/50\tTrain Loss: -0.947291\tTest Loss: -0.950297\n",
      "Epoch:  16/50\tTrain Loss: -0.950029\tTest Loss: -0.951817\n",
      "Epoch:  17/50\tTrain Loss: -0.951102\tTest Loss: -0.953713\n",
      "Epoch:  18/50\tTrain Loss: -0.952657\tTest Loss: -0.955618\n",
      "Epoch:  19/50\tTrain Loss: -0.955040\tTest Loss: -0.956757\n",
      "Epoch:  20/50\tTrain Loss: -0.956023\tTest Loss: -0.958202\n",
      "Epoch:  21/50\tTrain Loss: -0.957258\tTest Loss: -0.958263\n",
      "Epoch:  22/50\tTrain Loss: -0.957642\tTest Loss: -0.960098\n",
      "Epoch:  23/50\tTrain Loss: -0.958702\tTest Loss: -0.960964\n",
      "Epoch:  24/50\tTrain Loss: -0.960369\tTest Loss: -0.962044\n",
      "Epoch:  25/50\tTrain Loss: -0.961197\tTest Loss: -0.962626\n",
      "Epoch:  26/50\tTrain Loss: -0.961776\tTest Loss: -0.963411\n",
      "Epoch:  27/50\tTrain Loss: -0.962382\tTest Loss: -0.963744\n",
      "Epoch:  28/50\tTrain Loss: -0.963226\tTest Loss: -0.964134\n",
      "Epoch:  29/50\tTrain Loss: -0.963745\tTest Loss: -0.965219\n",
      "Epoch:  30/50\tTrain Loss: -0.964282\tTest Loss: -0.965691\n",
      "Epoch:  31/50\tTrain Loss: -0.964945\tTest Loss: -0.965961\n",
      "Epoch:  32/50\tTrain Loss: -0.965581\tTest Loss: -0.966416\n",
      "Epoch:  33/50\tTrain Loss: -0.966063\tTest Loss: -0.967110\n",
      "Epoch:  34/50\tTrain Loss: -0.966731\tTest Loss: -0.967437\n",
      "Epoch:  35/50\tTrain Loss: -0.967178\tTest Loss: -0.968195\n",
      "Epoch:  36/50\tTrain Loss: -0.967736\tTest Loss: -0.968547\n",
      "Epoch:  37/50\tTrain Loss: -0.968036\tTest Loss: -0.969126\n",
      "Epoch:  38/50\tTrain Loss: -0.968469\tTest Loss: -0.969252\n",
      "Epoch:  39/50\tTrain Loss: -0.968906\tTest Loss: -0.969538\n",
      "Epoch:  40/50\tTrain Loss: -0.969314\tTest Loss: -0.970329\n",
      "Epoch:  41/50\tTrain Loss: -0.969712\tTest Loss: -0.970831\n",
      "Epoch:  42/50\tTrain Loss: -0.970047\tTest Loss: -0.971067\n",
      "Epoch:  43/50\tTrain Loss: -0.970450\tTest Loss: -0.971134\n",
      "Epoch:  44/50\tTrain Loss: -0.970655\tTest Loss: -0.971823\n",
      "Epoch:  45/50\tTrain Loss: -0.971018\tTest Loss: -0.971751\n",
      "Epoch:  46/50\tTrain Loss: -0.971277\tTest Loss: -0.972265\n",
      "Epoch:  47/50\tTrain Loss: -0.971661\tTest Loss: -0.972292\n",
      "Epoch:  48/50\tTrain Loss: -0.971912\tTest Loss: -0.972309\n",
      "Epoch:  49/50\tTrain Loss: -0.972176\tTest Loss: -0.972485\n",
      "Epoch:  50/50\tTrain Loss: -0.972487\tTest Loss: -0.972970\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 510x850 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: /Users/original/Desktop/Johnson_Lab/image_registration/saved_models/simple_cnn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/original/Desktop/Johnson_Lab/image_registration/saved_models/simple_cnn/assets\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    class Args():\n",
    "      batch_size = 8\n",
    "      epochs = 50\n",
    "      lr = 0.004\n",
    "      label = 7  # which digit images to train on?\n",
    "      num_samples = 5  # number of sample results to show\n",
    "      save_model = True\n",
    "    \n",
    "    args = Args()\n",
    "    main(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "2024-02-22 02:22:52.649080: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-02-22 02:22:52.822441: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to ONNX and saved as /Users/original/Desktop/Johnson_Lab/image_registration/saved_models/MNIST_cnn.onnx\n"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "\n",
    "# Load the trained Keras model\n",
    "model_path = '/Users/original/Desktop/Johnson_Lab/image_registration/saved_models/simple_cnn'  \n",
    "keras_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "onnx_model_path = \"/Users/original/Desktop/Johnson_Lab/image_registration/saved_models/MNIST_cnn.onnx\"\n",
    "\n",
    "# Convert the Keras model to ONNX format\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(keras_model, opset=13, output_path=onnx_model_path)\n",
    "\n",
    "print(f\"Model converted to ONNX and saved as {onnx_model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
